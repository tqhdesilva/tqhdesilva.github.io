---
layout: post
title:  "Variational Auto-Encoders"
date:   2021-06-26 21:00:00 -0700
categories: vae
mathjax: true
---
Variational Auto-Encoders(VAE) are an extension of Auto-Encoders(AE) which allow a probabilistic representation of the latent space.
Rather than learning a deterministic latent representation of the data, VAE instead learns a distribution over the latent space.
This blog post is intended as a quick and dirty introduction to understanding and working with VAE.
We will not go into too much detail on the math or new developments and applications of VAE, instead focusing on showing code examples applied to MNIST.
First we'll give a brief overview and some examples of AE and then introduce VAE.

* TOC
{:toc}

# Auto-Encoder Overview
Auto-Encoders(AE) are a type of unsupervised model that aims to learn a latent representation of the data.
An advantage to learning a latent representation might be to leverage unlabeled data to generate features for some other task, or to decrease the dimensionality of the features, as the latent space is usually chosen to be lower dimensional than the input space.
In some sense, AE can be thought of as similar to Prinicipal Component Analysis(PCA), a method which also seeks to learn a latent representation.
Actually, if we restrict our encoder and decoder networks to be linear(i.e. no activation function, fully connected layers), then the optimal solution would be the same as PCA!

![Autoencoder](/assets/images/autoencoder.png)

AE are composed of two component networks: the encoder and the decoder.
The encoder maps the input space to the latent space, and the decoder does the inverse of this and maps the latent space to the input space.
Although the decoder inverts the encoder, it does not need to have the same architecture.
For example, our input network could be convolutional, but we could choose any architecture, such as fully connected, for our decoder.
The loss used to train the network is some form of reconstruction loss, which evaluates the reconstructed $\hat{x}$ against $x$, e.g. Mean-Square Error(MSE).

The encoder $f$ gives us $z = f(x)$ and the decoder $g$ gives us $\hat{x} = g(z)$.
What can we use the latent features $z$ and the reconstruction $\hat{x}$ for?

The latent feature $z$ could be used for any number of things.
Auto-Encoders could help us generate low dimensional representations of our data, which could be useful for visualization.
While we could fit a PCA model on the raw image data, this might not be a great idea since image data is highly non-linear.
Here is an example of PCA run on the latent features $z$ output from the encoder.

{% include image.html url="/assets/images/latent_pca.png" description="Example of 2D PCA Representation of Latent Features of MNIST." %}

The numbers look a bit mixed up still, but a few labels such as 1 and 9 form some more coherent clusters.
Keep in mind also, this only takes the first two principal components, i.e. good separation of all classes is incredibly hard to achieve in just 2 dimensions.

Looking at the reconstruction $\hat{x}$ is important for qualitatively assessing the performance of an Auto-Encoder.
Here we can see the original and reconstructed images for the MNIST Auto-Encoder.

{% include image.html url="/assets/images/reconstruction_ae.png" description="Original(left) and reconstructed(right) MNIST images." %}

In addition to assessing the model quality, the reconstruction $\hat{x}$ can also be used for collaborative recommendation systems and anomaly detection.
For collaborative recommendation systems, such as in [\[1\]](#kuchaiev_et_al), the model is trained to encode the sparse input $x$ into a latent space.
Then the reconstructed $\hat{x}$ is encouraged by a specific loss function, Masked Mean-Square Error(MMSE), to be dense.
The dense $\hat{x}$ can be used to predict what products a user will like.
In the case of anomaly detection, $\hat{x}$ and $x$ can be used to calculate the reconstruction error.
Samples with high reconstruction error may be deemed to be anomalous, as the model is more likely to perform better reconstruction on samples close to the manifold of training samples(the non-anomalous data).

Before moving on, let's talk about how flexible AE are and how they can be used in various contexts.
The architecture for AE is very general, and we have a lot of freedom to choose our encoder and decoder structure based on the task.
For example, although the above 2D PCA figure and the reconstruction example are from a fully connected AE, we could easily modify the network to use convolutional layers.
Another example of the flexibility of AE is how we can use recurrent networks for encoder and/or decoder.
In fact, if you've ever used a sequence to sequence model, such as in a translation task, you've seen what is essentially an AE for sequences.
The encoder in this case is a RNN, which takes in a sequence.
The decoder is also an RNN, which feeds it's output back into itself.
The latent state $z$ is the hidden state that gets passed from the last encoding RNN cell to the first decoding RNN cell.
$z$ should contain all the information needed to reconstruct the sequence.

{% include image.html url="/assets/images/RNN-AE.png" description="RNN Sequence-to-Sequence Model as AE" %}

Now that we've reviewed some basics about AE and how they are used, let's introduce Variational Auto-Encoders.

# Variational Auto-Encoder Overview
Variational Auto-Encoder(VAE) was introduced in the seminal paper by Kingma and Welling [\[2\]](#kingma_et_al) as a way to apply Stochastic Variational Inference(SVI) to training a probabilistic Auto-Encoder.
The idea behind VAE is still much the same as the AE case.
We will still encode the data using an encoder network, and reconstruct the input from the latent space using a decoder network.
However, the key difference is that the latent features that the encoder maps to are no longer deterministic.
In other words, instead of learning a deterministic function $x \rightarrow f(x)$, our goal is to learn a conditional distribution $p(z|x)$.

![VAE](/assets/images/VAE.png)

Unfortunately, in the general case finding the posterior $p(z|x)$ is not tractable.
The term variational describes the fact that we will approximate $p(z|x)$ using a variational distribution $q(z|x)$, which comes from a simpler family of distributions.
Often in the case of VAE, the family of distributions that $q$ is constrained to is the class of multivariate normal distributions with diagonal variance.

Why might we want to use VAE instead of regular AE?
Well, for one the model is Bayesian.
The posterior $p(z|x)$ is calculated as:

$$
p(z|x) = \frac{p(x|z) p(z)}{\int_{\mathbb{R}^n} p(x|z) p(z) dz}
$$

As a result, we have additional control over how our model learns the latent representation.
Also, as we will see in the next section, one part of the loss is a regularization term that penalizes learning distributions far from the prior.
We can control the amount of regularization by weighting the reconstruction loss vs the regularization loss from deviating from the prior.
Such an approach is known as $\beta$-VAE, for the hyperparameter that determines the weighting, $\beta$.
Finally, VAE can be used as a generative model.
By sampling from the prior distribution and passing the sampled latent space samples through the decoder, we can sample from $p(x)$.

For a more in-depth tutorial and overview of VAE, see [\[3\]](#doersch).

# VAE Loss and Training
The goal of training VAE is to simultaneously optimize the encoder to learn a good approximation $q(z|x)$ to $p(z|x)$ and at same time learn how to reconstruct $x$ from the latent features.
The decoder will determine our likelihood distribution $p(x|z)$.

The way to determine whether $q(z\|x)$ is a good approximation of $p(z\|x)$ is to use the Kullback-Leibler Divergence(KLD).
The KLD from probability measures $Q$ to $P$ is written as $D_{KL}(P\|\|Q)$.
KLD is asymmetric, meaning $D_{KL}(P\|\|Q) = D_{KL}(Q\|\|P)$ is not in general true.
In the general case, KLD is defined as:

$$
D_{KL}(P||Q) = \int_{\Omega} log(\frac{dP}{dQ}) dP
$$

for $P$, $Q$ measures on $\Omega$ and $P$ absolutely continuous w.r.t to $Q$.

The interpretation of KLD is that is that $D_{KL}(P||Q)$ describes the amount of information lost when using $Q$ to approximate $P$.
So using KLD can give us a way of quantifying how close $q(z|x)$ is to $p(z|x)$.
We'll be using KLD as the loss of the network, seeking to minimize:

$$
\begin{aligned}
&\min_{\theta, \phi} D_{KL}(q_\phi(z|x)||p(z|x)) \\
&= \min_{\theta, \phi} \int_{\mathbb{R}^n} q_\phi (z|x) \log \frac{q_\phi (z|x)}{p(z|x)} dz \\
&= \min_{\theta, \phi} \int_{\mathbb{R}^n} q_\phi (z|x) \log \frac{q_\phi (z|x) p(x)}{p(z, x)} dz \\
&= \min_{\theta, \phi} \int_{\mathbb{R}^n} q_\phi(z|x) (\log \frac{q_\phi(z|x)}{p(z, x)} + \log p(z)) dz \\
&= \min_{\theta, \phi} \int_{\mathbb{R}^n} q_\phi(z|x) \log \frac{q_\phi(z|x)}{p(z, x)} dz + \int_{\mathbb{R}^n} q_\phi(z|x) \log p(x) dz \\
&= \log p(x) + \min_{\theta, \phi} \int_{\mathbb{R}^n} q_\phi(z|x) \log \frac{q_\phi(z|x)}{p(z, x)} dz
\end{aligned}
$$

where $\phi$ are the parameters of the encoder and $\theta$ are the parameters of the decoder.
Note that although the expression does not contain $\theta$ at the moment, it will appear once $p_\theta(x|z)$ appears.
In the last step, $p(x)$ in the sum does not depend on either parameter $\theta$, $\phi$ so we can safely ignore it.


# MNIST VAE Example

```python
class VAE(nn.Module):
    def __init__(self, latent_size):
        super(VAE, self).__init__()
        self.latent_size = latent_size
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),
            nn.AvgPool2d(kernel_size=2),
            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),
            nn.AvgPool2d(kernel_size=2),
            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5),
            nn.Flatten(),
            nn.Linear(120, 84),
            nn.Tanh(),
            nn.Linear(84, 2 * latent_size),
            nn.Tanh(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_size, 84),
            nn.Tanh(),
            nn.Linear(84, 120),
            nn.Tanh(),
            nn.Unflatten(dim=1, unflattened_size=(120, 1, 1)),
            nn.ConvTranspose2d(in_channels=120, out_channels=16, kernel_size=5),
            nn.ReLU(),
            nn.ConvTranspose2d(
                in_channels=16, out_channels=16, kernel_size=2, stride=2
            ),
            nn.ReLU(),
            nn.ConvTranspose2d(in_channels=16, out_channels=6, kernel_size=5, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(in_channels=6, out_channels=1, kernel_size=6),
        )

    def forward(self, input):
        encoder_out = self.encoder(input)
        mu, logvar = torch.split(encoder_out, self.latent_size, dim=-1)
        z = mu + torch.randn(mu.shape).to(input.device) * torch.exp(logvar)
        output = self.decoder(z)
        return output, mu, logvar

```

```python

def kld_loss(mu, logvar):
    loss = 0.5 * torch.mean(torch.exp(logvar) + mu ** 2 - 1 - logvar)
    return loss


def reconstruction_loss(x_est, x_actual):
    return nn.functional.mse_loss(x_est, x_actual)


def elbo_loss(x_est, x_actual, mu, logvar):
    return reconstruction_loss(x_est, x_actual) + kld_loss(mu, logvar)
```

{% include image.html url="/assets/images/vae_pca.png" description="First two principal components of latent features from VAE." %}

{% include image.html url="/assets/images/vae_reconstruction_2.png" description="Original(left), VAE reconstruction(right) on MNIST" %}

# References

<a name="kuchaiev_et_al" class="aref">[1] Oleksii Kuchaiev and Boris Ginsburg. [Training Deep AutoEncoders for Collaborative Filtering](https://arxiv.org/abs/1708.01715).</a>

<a name="kingma_et_al" class="aref">[2] Diederik Kingma and Max Welling. [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)</a>

<a name="doersch" class="aref">[3] Carl Doersch. [Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908).</a>
