# Neural Tangent Kernel

Neural networks in the infinite width limit can be considered as equivalent to kernel regression.

In kernel regression, instead of using gradient descent $\theta_{t + 1} = \theta_t - \eta \nabla_\theta L(x, y; \theta_t, f)$, we instead take steps in the function space $f_{t + 1} = f_t - \eta \nabla_K C|_{f_t}$, independent of how we parametrize the function $f$ where $C$ is a convext cost function. The functional gradient is depending only on the cost function $C$ and the kernel $K$.

Being able to re-frame training a neural network trained with gradient descent as kernel descent is useful when we want to analyze the properties of the neural network in terms of the constant limiting kernel of kernel gradient descent.
To do this, we must define a kernel such that gradient descent and kernel gradient descent are equivalent, and then show that there is a constant limiting kernel as $P \rightarrow \infty$.




- Kernel gradient descent and gradient descent are equivalent.
    - Show $\partial_t f_{\theta(t)}  = - \nabla_{\tilde{K}} C |_{f_{\theta(t)}}$.
- In the random function approximation setting, where the prediction function is a sum of randomly selected functions weighted by the optimized params $\theta$, there is a limiting kernel as $P \rightarrow \infty$.
    - The kernel does not depend on the values of $\theta$ at all, just $f^{(l)}$ the random basis functions.
    $$
    \tilde{K}(x, x') = \frac{1}{P} \sum_{p=1}^{P} f^{(p)} (x)\otimes f^{(p)}(x')
    $$
    $\lim_{P \rightarrow \infty} \tilde{K} = K$ by LLN.
    
- In the case of deep neural networks with some Lipschitz activation function, the kernel function does depend on the values of the parameters $\theta$, activations, and initialization.
    - In order to show the kernel is invariant in the limit as $P \rightarrow \infty$, we must show that it is the limiting kernel at initialization, and then 



## Kernel Regression
Consider a function $f$ which can be defined in terms of the weighted sum of various kernels centered at different locations $x_i$:

$$
f(x) = \sum_i K(x^{(i)}, x) \alpha^{(i)}
$$

$f: \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_l}$, $K: \mathbb{R}^{n_0} \times \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_l} \times \mathbb{R}^{n_l}$, $\alpha_i \in \mathbb{R}^{n_l}$



Such a function $f$ is said to be an element of a Reproducing Hilbert Kernel Space (RKHS) $\mathcal{H}_K$.


## Kernel Gradient
Consider a cost function in the dual space $C \in \mathcal{H}_K^*: \mathcal{H}_K \rightarrow \mathbb{R}$.
Then $C \circ f$ is analagous to the loss function, evaluating $f$ over the finite dataset $x_1, \ldots, x_n \in \mathbb{R}^{n_0}$.

The kernel gradient for kernel $K$ is written $\nabla_K C$ and defines a mapping $\mathcal{H}_K^* \rightarrow \mathcal{H}_K$.
Evaluated at a function $f_0$, we are actually evaluating $\nabla_K C |_{f_0} = $

## References
